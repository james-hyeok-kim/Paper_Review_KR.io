# SVDQUANT: ABSORBING OUTLIERS BY LOW-RANK COMPONENTS FOR 4-BIT DIFFUSION MODELS

저자: 

Muyang Li1∗ ‡ Yujun Lin1∗ Zhekai Zhang1† Tianle Cai4 Xiuyu Li5‡

Junxian Guo1,6 Enze Xie2 Chenlin Meng7 Jun-Yan Zhu3 Song Han1,2

1MIT 2NVIDIA 3CMU 4Princeton 5UC Berkeley 6SJTU 7Pika Labs

출간: ICLR 2025, arXiv 버전(v4)은 2025년 11월 8일에 업데이트

논문: [PDF](https://arxiv.org/pdf/2411.05007)

---

## 1. Introduction


### 1. 배경 및 당면 과제: 디퓨전 모델의 대형화

<p align = 'center'>
<img width="250" height="350" alt="image" src="https://github.com/user-attachments/assets/3281bea2-736e-409a-9b8e-baa4de6ddc80" />
</p>

* 모델 규모의 확장: 최근 디퓨전 모델은 시각적 품질과 텍스트 정렬 능력을 높이기 위해 모델 크기를 급격히 키우고 있습니다.
    * 예를 들어, Stable Diffusion 1.4(800M)에서 시작해 SDXL(2.6B), AuraFlow(6B), 그리고 최신 FLUX.1(12B)에 이르기까지 매개변수 수가 크게 증가
* 높은 연산 집약도: 디퓨전 모델은 대규모 언어 모델(LLM)보다 연산량이 훨씬 많으며, 모델 크기가 커짐에 따라 연산 비용이 더 빠르게 증가
* 연산 병목 현상(Compute-bounded): LLM의 지연 시간이 주로 가중치를 로드하는 메모리 대역폭에 의해 결정되는 것과 달리, 디퓨전 모델은 단일 배치에서도 연산 자체가 병목이 되는 특성을 가집니다. 따라서 속도를 높이려면 가중치뿐만 아니라 활성화(activation)까지 모두 낮은 비트로 양자화

### 2. 기존 방식의 한계

* 품질 저하 문제: 가중치와 활성화를 모두 4비트로 양자화(W4A4)하는 공격적인 방식은 심각한 이미지 품질 저하를 초래하기 쉽습니다.

* Smoothing 방식의 부족함: 가중치와 활성화 사이의 이상치(outlier)를 재분배하는 기존의 '스무딩(smoothing)' 기법은 양쪽 모두가 이상치에 민감한 상황에서는 충분한 효과를 거두지 못합니다.


### 3. 제안 방법: SVDQuant 패러다임

<p align = 'center'>
<img width="772" height="273" alt="image" src="https://github.com/user-attachments/assets/a2bb174b-01e9-400e-90c1-7f08117fbf8b" />
</p>

* 이상치 통합 및 이동: 먼저 스무딩 기법을 통해 활성화의 이상치를 가중치로 이동시켜 통합합니다.
* SVD를 이용한 분해: 업데이트된 가중치를 특잇값 분해(SVD)하여, 큰 이상치를 포함하는 16비트 고정밀 저차원 분기(low-rank branch)와 나머지 잔차를 처리하는 4비트 양자화 분기로 나눕니다.
* Nunchaku 추론 엔진: 독립적인 저차원 분기 실행으로 인한 메모리 오버헤드를 막기 위해, 두 분기의 커널을 하나로 합쳐 데이터 이동을 최소화하는 전용 엔진 'Nunchaku'를 공동 설계했습니다.

### 4. 주요 성과

* 메모리 절감: 12B 규모의 FLUX.1 모델의 메모리 사용량을 3.5배 줄였습니다.
* 속도 향상: 16GB VRAM을 가진 노트북용 RTX 4090 GPU에서 CPU 오프로딩 없이 실행 가능하게 하여, 16비트 모델 대비 8.7배, 기존 4비트 가중치 전용 양자화(W4A16) 대비 3배의 속도 향상을 달성했습니다.
* 범용성: UNet 및 DiT 구조 모두에서 시각적 품질을 잘 유지하며, 재양자화 없이 기존 LoRA와도 원활하게 통합됩니다.


---

## 2. Related Work

### 1. 디퓨전 모델 및 가속화 (Diffusion Models & Acceleration)

* 모델의 진화: 초기 컨볼루션 기반의 UNet 구조에서 최근에는 트랜스포머 기반의 DiT 아키텍처로 전환되고 있으며, 모델의 규모가 계속 커지고 있습니다.
* 속도 개선 노력: 느린 추론 속도를 해결하기 위해 적은 단계로 이미지를 생성하는 샘플러(Few-step samplers) 연구나 모델을 압축하는 증류(Distillation) 기법이 활발히 연구되어 왔습니다.
* 기타 최적화: 효율적인 아키텍처 설계, 희소 추론(Sparse inference), 분산 추론 등 다양한 하드웨어 가속 방식이 제안되었습니다.

### 2. 양자화 기법 (Quantization)

* LLM에서의 활용: 모델 크기를 줄이고 추론을 가속화하기 위해 LLM 분야에서 양자화가 널리 사용되어 왔습니다.
* 디퓨전 모델 양자화: 초기에는 8비트 양자화(Q-Diffusion, PTQ4DM 등)가 주를 이루었으며, 이후 타임스텝을 고려한 양자화나 비디오 생성 모델용 양자화 등으로 발전했습니다.
* SVDQuant의 진보: 기존 연구들이 주로 8비트 수준에 머물거나 실제 속도 향상을 보고하지 못한 것과 달리, SVDQuant는 4비트 양자화(W4A4)를 성공적으로 적용하고 실제 GPU에서의 속도 향상을 입증한 최초의 사례 중 하나입니다.

### 3. 저차원 분해 (Low-rank Decomposition)

* 압축 및 튜닝: 저차원 분해는 연산 효율을 높이거나 LoRA와 같이 효율적인 모델 파인튜닝을 위해 널리 쓰여왔습니다.
* 기존 저차원 양자화와의 차이: 이전 연구(예: LoRC)는 양자화 오류를 보상하기 위해 저차원 분기를 사용했지만, 주로 가중치에만 집중하여 실제 추론 속도 향상이 미미했습니다.
* 공동 설계(Co-design): SVDQuant는 가중치와 활성화를 함께 양자화하며, 별도 분기로 인한 오버헤드를 줄이기 위해 Nunchaku 추론 엔진을 통해 커널을 융합했다는 점이 핵심적인 차별점입니다.

---

## 3. Quantization Preliminary

### 1. 양자화의 기본 정의

* 양자화 공식: 텐서 $X$의 양자화 표현 $Q_X$는 다음과 같이 정의됩니다.

$$Q_X = \text{round}\left(\frac{X}{s_X}\right)$$

* 스케일링 인자 ($s_X$): 데이터의 범위를 양자화 범위에 맞추기 위해 사용됩니다.

$$s_X = \frac{\max(X)}{q_{max}}$$

* $q_{max}$ 값: $k$-비트 부호 있는 정수(INT)의 경우 $q_{max} = 2^{k-1} - 1$이며, 4비트 부동소수점(FP4)의 경우 6으로 설정됩니다.


### 2. 선형 레이어 연산의 근사

$$XW \approx Q(X)Q(W) = s_X s_W \cdot Q_X Q_W$$

### 3. 하드웨어 가속을 위한 제약 조건

* 동일 비트 폭 요구: 현대의 상업용 GPU에서 연산 속도를 높이려면 입력( $Q_X$ )과 가중치( $Q_W$ )가 동일한 비트 폭을 사용해야 합니다.
* 업캐스팅(Upcast)의 문제: 만약 비트 폭이 다르면 연산 중에 낮은 정밀도의 데이터를 높은 정밀도로 변환해야 하므로, 양자화로 얻을 수 있는 성능 이점이 사라지게 됩니다.
* 용어 정의: 논문에서는 $z$-비트 가중치와 $y$-비트 활성화를 $WzAy$로 표기하며, 본 연구의 목표는 W4A4(4비트 가중치 및 활성화) 가속입니다.

### 4. 기존 기법의 한계

* W4A4와 같은 공격적인 양자화에서 발생하는 이상치(Outlier) 문제를 해결하기 위해 기존에 사용되던 방법들의 한계점은 다음과 같습니다.

#### 양자화 인식 훈련 (QAT)
* FLUX.1과 같이 10B 이상의 매개변수를 가진 대형 모델을 튜닝하기에는 막대한 컴퓨팅 자원이 필요함.

#### 회전 (Rotation)

* "디퓨전 모델의 적응형 정규화(Adaptive Normalization) 레이어 때문에 오프라인 적용이 불가능하며, 온라인 적용은 실행 오버헤드가 큼."


---
