# Denoising Diffusion Probabilistic Models - 1
ì €ì(ì†Œì†) : Jonathan Ho (UC Berkeley), Ajay Jain(UC Berkeley), Pieter Abbeel(UC Berkeley)

ë…¼ë¬¸ : [PDF](https://arxiv.org/pdf/2006.11239)

ì¼ì : 16 Dec 2020

## í•µì‹¬ ì•„ì´ë””ì–´

### ìˆœë°©í–¥ í™•ì‚°(forward process): ì‹¤ì œ ë°ì´í„° 
* $ğ‘¥_0$â€‹ì— Gaussian ë…¸ì´ì¦ˆë¥¼ ì ì§„ì ìœ¼ë¡œ ì¶”ê°€í•˜ì—¬ ë‹¨ê³„ë³„ë¡œ $ğ‘¥_1,ğ‘¥_2, ... , ğ‘¥_ğ‘‡$ ë¥¼ ìƒì„±.
* $ğ‘‡$ê°€ ì¶©ë¶„íˆ í¬ë©´ $ğ‘¥_ğ‘‡$ëŠ” ê±°ì˜ ì •ê·œë¶„í¬ $ğ‘(0,ğ¼)$ì™€ ìœ ì‚¬í•´ì§ 

### ì—­ë°©í–¥ ê³¼ì •(reverse process)
* ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ì—ì„œ ì ì°¨ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ì—¬ ì›ë³¸ ë°ì´í„°ë¥¼ ë³µì›í•˜ëŠ” í™•ë¥ ì  ê²½ë¡œë¥¼ í•™ìŠµ.
* ì´ ê³¼ì •ì´ ë°ì´í„° ìƒì„±ì˜ í•µì‹¬ 

## ì´ˆë¡
Implementation : [Git](https://github.com/hojonathanho/diffusion)

## ë„ì…
<p align="center">
<img src = "https://github.com/user-attachments/assets/1351575a-8638-446c-9a9b-d5d9dc8db15c" width="60%" height="60%">
</p> 

Markov chain forwarding ë°©ì‹ìœ¼ë¡œ noiseë¥¼ ë”í•˜ê³ , reverseë°©ì‹ìœ¼ë¡œ noiseì—ì„œ ì´ë¯¸ì§€ë¥¼ ìƒì„±

## ë°°ê²½
### Reverse Process $p_{\theta}$
* $p_{\theta}(x_{0:T}) \rightarrow reverse \ process$
* Markov chain with learned Gaussian transitions, $p(x_T) = N(x_T;0,I):$ (Normal distribution)
* ë³´í†µ Normal Distributionì˜ í‘œí˜„ $X \sim N(\mu, \sigma^2)$ í‰ê·  $(\mu)$ , ë¶„ì‚° $(\sigma)$ ë¡œ í‘œí˜„
* $p_{\theta}(x_{0:T}) := p(x_{T})\displaystyle\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t}),  \ \ \ p_{\theta}(x_{t-1}|x_t) :=  N (x_{t-1};\mu_{\theta}(x_t,t),\sum_{\theta}(x_t,t))$

### Forward Process (Diffusion Process) $q$
* $q(x_{1:T}|x_0) := \displaystyle\prod_{t=1}^{T}q(x_t|x_{t-1}), \ \ \ q(x_t|x_{t-1}) := N(x_t;\sqrt{1- \beta_{t}}x_{t-1},\beta_{t}I)$
* Variance(Noise) Schedule $\beta_1, ... , \beta_T:$ - ë¯¸ë¦¬ ì •í•´ë‘” ë…¸ì´ì¦ˆê°’
* $\sqrt{1- \beta_{t}}$ ë¡œ scalingí•˜ëŠ” ì´ìœ ëŠ” varianceê°€ ë°œì‚°í•˜ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ì„œ

### Training (í•™ìŠµ)
* Variational Boundë¥¼ ìµœì í™” í•˜ëŠ” í˜•íƒœë¡œ ì§„í–‰
* Negative log likelihood
* $E\left[ -log p_\theta(x_0) \right] \le E_q \left[ -log \frac{p_\theta (x_{0:t})}{q(x_{1:T}|x_0)} \right] = E_q \left[ -log p(x_T) - \displaystyle\sum_{t \ge 1} log \frac{p_\theta (x_{t-1})}{q(x_{t}|x_{t-1})} \right] =: L$
* $E$ëŠ” ê¸°ëŒ€ê°’, ë³€ìˆ˜ xì˜ ê¸°ëŒ€ê°’ìœ¼ë¡œ ë°˜ë³µì‹¤í—˜ì˜ í‰ê· ì ì¸ ê°’ì„ ì˜ë¯¸

### ì¶”ê°€ì ì¸ ì¡°ê±´
* sampling $x_t$ at arbitrary timestep t
* $\alpha_t := 1-\beta_t$
* $\bar{\alpha_t}=\prod^t_{s=1}\alpha_s$
* $q(x_t|x_0) = N(x_t;\sqrt{\bar{a_t}}x_0, (1-\bar{\alpha_t})I)$


## ì‹¤í—˜

## ê²°ê³¼

## ë¶€ë¡
### Markov Chain
#### ë§ˆë¥´ì½”í”„ ì„±ì§ˆ + ì´ì‚°ì‹œê°„ í™•ë¥  ê³¼ì •
ë§ˆë¥´ì½”í”„ ì²´ì¸ì€ 'ë§ˆë¥´ì½”í”„ ì„±ì§ˆ'ì„ ê°€ì§„ 'ì´ì‚°ì‹œê°„ í™•ë¥ ê³¼ì •' ì…ë‹ˆë‹¤.
ë§ˆë¥´ì½”í”„ ì„±ì§ˆ - ê³¼ê±°ì™€ í˜„ì¬ ìƒíƒœê°€ ì£¼ì–´ì¡Œì„ ë•Œì˜ ë¯¸ë˜ ìƒíƒœì˜ ì¡°ê±´ë¶€ í™•ë¥  ë¶„í¬ê°€ ê³¼ê±° ìƒíƒœì™€ëŠ” ë…ë¦½ì ìœ¼ë¡œ í˜„ì¬ ìƒíƒœì— ì˜í•´ì„œë§Œ ê²°ì •ë¨
ì´ì‚°ì‹œê°„ í™•ë¥ ê³¼ì • - ì´ì‚°ì ì¸ ì‹œê°„ì˜ ë³€í™”ì— ë”°ë¼ í™•ë¥ ì´ ë³€í™”í•˜ëŠ” ê³¼ì •
<p align="center">
<img src = "https://github.com/user-attachments/assets/7ae5afbc-7884-4e35-a570-cb87513daaf7" width="40%" height="40%">
</p> 

#### ê²°í•©í™•ë¥ ë¶„í¬(Joint Probability Distribution)
ì˜ˆë¥¼ ë“¤ì–´ í™•ë¥  ë³€ìˆ˜ $X_1,X_2, ... , X_n$ ì´ ìˆë‹¤ê³  ê°€ì •í•˜ë©´,
ì¼ë°˜ì ìœ¼ë¡œ ì´ í™•ë¥ ë³€ìˆ˜ë“¤ì˜ ê²°í•©í™•ë¥ ë¶„í¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

$$ P(X_1,X_2, ... , X_n) = P(X_1) \times P(X_2|X_1) \times P(X_3|X_2,X_1)\times  ...  \times P(X_n|X_{n-1}, X_{n_2} , ... , X_1) $$
 
í•˜ì§€ë§Œ ë§ˆë¥´ì½”í”„ ì„±ì§ˆì„ ì´ìš©í•˜ë©´ ìœ„ ë³´ë‹¤ ë” ë‹¨ìˆœí•œ ê³„ì‚°ì„ í†µí•´ ê²°í•©í™•ë¥ ë¶„í¬ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.

$$ P(X_n|X_{n-1}, X_{n_2} , ... , X_1) = P(X_{t+1}|X_t) $$
 

ë§Œì•½ ì–´ë– í•œ ìƒíƒœì˜ ì‹œì ì´ê³ , í™•ë¥ ë¶„í¬ê°€ ë§ˆë¥´ì½”í”„ ì„±ì§ˆì„ ë”°ë¥¸ë‹¤ë©´ 

$$ P(X_1,X_2, ... , X_n) = P(X_1) \times P(X_2|X_1) \times P(X_3|X_2)\times  ...  \times P(X_n|X_{n-1}) $$

ë‹¨ìˆœí™” í•  ìˆ˜ ìˆê³  ì¼ë°˜í™”ë¥¼ ì ìš©í•˜ë©´ ì´ì „ì— ê²°í•©í™•ë¥ ë¶„í¬ì˜ ê³„ì‚°ì„ ë‹¤ìŒê³¼ ê°™ì´ ë‹¨ìˆœí™” ê°€ëŠ¥í•˜ë‹¤.

### variational bound
* VAE(Variational Auto-Encoder)ì—ì„œ ì“°ì´ëŠ” ê°œë…ìœ¼ë¡œ ì‹¤ì œ ë¶„í¬ì™€ ì¶”ì • ë¶„í¬ ë‘ ë¶„í¬ ì‚¬ì´ì˜ ê±°ë¦¬ì¸ KL divergenceë¥¼ ìµœì†Œí™” ì‹œí‚¤ê¸° ìœ„í•´ ë„ì…ë˜ëŠ” ê°œë…ì…ë‹ˆë‹¤.
#### Variational Inference
* Variational Methodì—ì„œ ìœ ë˜
* ë³µì¡í•œ ë¬¸ì œë¥¼ ê°„ë‹¤í•œ ë¬¸ì œë¡œ ë³€í™”ì‹œì¼œ ê·¼ì‚¬
* Variational Parameterë³€ìˆ˜ë¥¼ ì¶”ê°€ë¡œ ë„ì…
* ì¶”ì • ë¬¸ì œë¥¼ ìµœì í™”

##### Example $\lambda$ ë„ì…í•˜ì—¬ log xë¥¼ ì§ì„ ìœ¼ë¡œ ê·¼ì‚¬
* $g(x) = log(x) \rightarrow f(x) = \lambda x - b(\lambda)$
* $f^*(\lambda) = \displaystyle\min_x \lbrace \lambda x - f(x) \rbrace$
* í™•ë¥  ë¶„í¬ë¡œ í™•ì¥ $q(X) = A(X, \lambda_{0}), (where \ \lambda_{0} = arg\displaystyle\max_{\lambda} \lbrace A(X_0,\lambda)\rbrace)$


##### Variational Inference ì‹ ìœ ë„
https://modulabs.co.kr/blog/variational-inference-intro

* $p(x)$ í™•ë¥ ë¶„í¬, ì€ë‹‰ë³€ìˆ˜ $Z$, ì–‘ë³€ì— logë¥¼ ì”Œìš°ë©´ Jensen ë¶€ë“±ì‹ì„ í†µí•´ Lower Bound í‘œí˜„
* $q(Z|\lambda)$ ì—ì„œ $\lambda$ ëŠ” Variational Parameter, $\lambda$ê°€ $q$ì— ì‘ë™í•œë‹¤ëŠ” í‘œí˜„
* $KL(p \parallel q) = \sum_Z p(Z) log p(Z) / q(Z)$ ë¡œ ì •ì˜, ë‘ í™•ë¥ ë¶„í¬ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜

$logp(X) = log(\displaystyle\sum_Z p(X,Z))$

$\ \ \ \ \ \ \ \ \ \ \ \ \  = log(\displaystyle\sum_Z p(X,Z)\frac{q(Z|\lambda)}{q(Z|\lambda)})$

$\ \ \ \ \ \ \ \ \ \ \ \ \  = log(\displaystyle\sum_Z q(Z|\lambda)\frac{p(X,Z)}{q(Z|\lambda)})$

$\ \ \ \ \ \ \ \ \ \ \ \ \  \ge \displaystyle\sum_Z q(Z|\lambda)log\frac{p(X,Z)}{p(Z|\lambda)}$
   

### 
